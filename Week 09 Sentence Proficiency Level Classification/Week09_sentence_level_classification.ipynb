{"cells":[{"cell_type":"markdown","metadata":{"id":"7L3cYlZl15_K"},"source":["# Week 9: Sentence Level Classification with BERT\n","\n","Your goal this week is to train a classifier that can predict the CEFR level of any given sentence. In this notebook we will guide you through the process of using ðŸ¤—[Hugging Face](https://huggingface.co/) and its transformers library as the training framework, with [Pytorch](https://pytorch.org/) as the deep learning backend, but feel free to use [TensorFlow](https://www.tensorflow.org) if that's what you are more familiar with.\n","\n","For this assignment we will provide a dataset containing sentences with the corresponding CEFR level, and you have to use BERT and train a sentence classifier with this dataset."]},{"cell_type":"markdown","metadata":{"id":"G8TbxtroCxM8"},"source":["## Prepare your environment\n","\n","As always, we highly recommend that you install all packages with a virtual environment manager, like [venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) or [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html), to prevent version conflicts of different packages.  "]},{"cell_type":"markdown","metadata":{"id":"wSHP0CPoXj7Z"},"source":["### Install CUDA\n","Deep learning is a computionally extensive process. It takes lots of time if relying only on the CPU, especially when it's trained on a large dataset. That's why using GPU instead is generally recommended.  \n","To use GPU for computation, you have to install [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit) as well as the [cuDNN library](https://developer.nvidia.com/cudnn) provided by NVIDIA.  \n","\n","If you already had CUDA installed on your machine, then great! You're done here.  \n","If you don't, you can refer to [Appendix](#Appendix-1-Install-CUDA) to see how to do so."]},{"cell_type":"markdown","metadata":{"id":"f78jLXeZfPyH"},"source":["\n","### Install python packages\n","The following python packages will be used in this tutorial:\n","\n","1. `numpy`: for matrix operation\n","2. `scikit-learn`: for label encoding\n","3. `datasets`: for data preparation\n","4. `transformers`: for model loading and finetuing\n","5. `pytorch`: the backend DL framework\n","  - Note that the pt version must support the CUDA version you've installed if you want to use GPU."]},{"cell_type":"markdown","metadata":{"id":"3cS9CxjyfQ-F"},"source":["### Select GPU(s) for your backend\n","\n","Skip this section if you have no intension of using GPU with tensorflow/pytorch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sEJ8Y8SCfWp_"},"outputs":[],"source":["import os\n","\n","# select your GPU. Note that this should be set before you load tensorflow or pytorch.\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","\n","# To use multiple GPUs, combine all GPU ID with commas\n","# e.g. >>> os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,3'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p1p_qQKbfcCH","outputId":"d25bdd87-959c-435d-8c57-22f5c350098f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/xu0494akk/miniconda3/envs/py36/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","# Check if any GPU is used\n","torch.cuda.is_available()\n"]},{"cell_type":"markdown","metadata":{"id":"-P0foxjBDQSu"},"source":["## Prepare the dataset\n","\n","Before starting the training, we need to load and process our dataset - but wait, let's decide which model we want to use first.  \n","\n","In the highly unlikely chance you've never heard of it, [BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) is a language model proposed by Google AI in 2018, and it's currently one of the most popular models used in NLP.  \n","You can learn more about it here:\n","- [BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/) by Samia, 2019.\n","\n","\n","However, we will not directly use BERT in this tutorial, because it's large and takes too long to train. Instead, we'll be using [DistilBert](https://medium.com/huggingface/distilbert-8cf3380435b5), a version of BERT that while light-weight, reserves 95% of its original accuracy.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lqb2cHCmDIEp"},"outputs":[],"source":["# the model you want to use. Available models can be found here: https://huggingface.co/models\n","MODEL_NAME = 'bert-base-uncased'"]},{"cell_type":"markdown","metadata":{"id":"QxSUKsTkDSxJ"},"source":["### Load data\n","\n","Similar to the `transformers` library, `datasets` is also a package by huggingface. It contains many public datasets online and can help us with the data processing.  \n","We can use `load_dataset` function to read the input `.csv` file provided for this assignment.\n","\n","Reference:\n"," - [Official datasets document](https://huggingface.co/docs/datasets)\n"," - [datasets.load_dataset](https://huggingface.co/docs/datasets/loading.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjY9HMNIt5jf","outputId":"65469e60-5cd4-4710-c9fb-49c92c3ab0c4"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using custom data configuration data-9169fe6d1c846a71\n","Reusing dataset csv (/home/xu0494akk/.cache/huggingface/datasets/csv/data-9169fe6d1c846a71/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 43.23it/s]\n"]}],"source":["# [ TODO ] load the data using the load_dataset function\n","\n","from datasets import load_dataset\n","dataset = load_dataset(\"/home/xu0494akk/nlplab/data\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"npepcv7GfMHI","outputId":"02ed2940-370f-444c-b748-abcd623891bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'level'],\n","        num_rows: 20720\n","    })\n","    test: Dataset({\n","        features: ['text', 'level'],\n","        num_rows: 2300\n","    })\n","})\n","Dataset({\n","    features: ['text', 'level'],\n","    num_rows: 20720\n","})\n","{'text': ['My mother is having her car repaired.', 'You can contact me by e-mail.', 'He had a break for the weekend, and he called me: \"I am in London, so, if you want to see me, it\\'s the time!\"', \"Research shows that 40 percent of the program's viewers are aged over 55.\", \"I'd guess she's about my age.\"], 'level': ['B1', 'A1', 'B1', 'B2', 'A1']}\n"]}],"source":["print(dataset)\n","print(dataset['train'])\n","print(dataset['train'][:5])\n"]},{"cell_type":"markdown","metadata":{"id":"E3olKw19uuJQ"},"source":["### Preprocessing\n","\n","As always, texts should be tokenized, embedded, and padded before being put into the model.  \n","But not to worry, there are libraries from huggingface to help with this, too."]},{"cell_type":"markdown","metadata":{"id":"x3ANJAV7wn2R"},"source":["#### Sentence processing\n","\n","Different pre-trained language models may have their own preprocessing models, and that's why we should use the tokenizers trained along with that model. In our case, we are using distilBERT, so we should use the distilBERT tokenizer.  \n","\n","With huggingface, loading different tokenizers is extremely easy: just import the AutoTokenizer from `transformers` and tell it what model you plan to use, and it will handle everything for you.\n","\n","Reference:\n"," - [transformers.AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DALrmSh6wpmy"},"outputs":[],"source":["# [ TODO ] load the distilBERT tokenizer using AutoTokenizer\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfz4pxxp6utW","outputId":"91de7390-72eb-472b-90fd-e520e4a108dd"},"outputs":[{"data":{"text/plain":["{'input_ids': [101, 2019, 7738, 2038, 2042, 2657, 2011, 1996, 13229, 2136, 1999, 25669, 1010, 2055, 3429, 2781, 2044, 2019, 2250, 8118, 19558, 2001, 5015, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["example = \"An explosion has been heard by the CNN team in Kyiv, about 45 minutes after an air raid siren was sounded.\"\n","\n","example_embeddings = tokenizer(example)\n","example_embeddings"]},{"cell_type":"markdown","metadata":{"id":"Jd43REmNxCOV"},"source":["#### Label processing\n","\n","Our labels also need to be processed, so let's do that next.\n","\n","For this tutorial, we'll use the OneHotEncoder provided by scikit-learn.\n","\n","For now, just declare a new encoder and use `fit` to learn the data. Hint: you should still end up with 6 labels.\n","\n","Documents:\n"," - [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3ml0_WxxFh5"},"outputs":[],"source":["# [ TODO ] declare a new encoder and let it learn from the dataset\n","import numpy as np\n","from sklearn.preprocessing import OneHotEncoder\n","\n","encoder = OneHotEncoder(sparse = False)\n","encoder = encoder.fit(np.reshape(dataset['train']['level'], (-1, 1)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BRY2GDJa1MxF","outputId":"df210570-a2f8-478d-8902-e6f6c4f33af5"},"outputs":[{"name":"stdout","output_type":"stream","text":["6\n"]}],"source":["# check if you still have 6 labels\n","LABEL_COUNT = len(encoder.categories_[0])\n","print(LABEL_COUNT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RlAD2oA56utY","outputId":"a6f524bd-12e1-47ea-df77-1a3b9db521e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["[array(['A1', 'A2', 'B1', 'B2', 'C1', 'C2'], dtype='<U2')]\n","[[1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 0. 1.]]\n","[['B1']]\n"]}],"source":["print(encoder.categories_)\n","print(encoder.transform([['A1'],['A2'],['B1'],['B2'],['C1'], ['C2']]))\n","\n","print(encoder.inverse_transform([[0, 0, 1, 0, 0, 0]]))"]},{"cell_type":"markdown","metadata":{"id":"qrfO8c4R1eWO"},"source":["#### Process the data\n","\n","To make things easier, we can write a function to process our dataset in batches. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvwnYLah1dbN"},"outputs":[],"source":["def preprocess(dataslice):    \n","    \"\"\" Input: a batch of your dataset\n","        Example: { 'text': [['sentence1'], ['setence2'], ...],\n","                   'label': ['label1', 'label2', ...] }\n","    \"\"\"\n","    # [ TODO ] use your tokenizor and encoder to get sentence embeddings and encoded labels\n","    embeddings = tokenizer(dataslice['text'])\n","    dataslice.update(embeddings)\n","    encoder = OneHotEncoder(sparse = False)\n","    encoder = encoder.fit(np.reshape(dataslice['level'], (-1, 1)))\n","    dataslice['label'] = encoder.transform(np.reshape(dataslice['level'], (-1, 1)))\n","    return dataslice\n","\n","    \n","    \"\"\" Output: a batch of processed dataset\n","        Example: { 'input_ids': ...,\n","                   'attention_masks': ...,\n","                   'label': ... }\n","    \"\"\"\n","   \n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nr0-Y1bR2efQ","outputId":"85efafc9-5341-41f5-c2f8-68526c282f8e"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/xu0494akk/.cache/huggingface/datasets/csv/data-9169fe6d1c846a71/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-c9d053e9f1ac937b.arrow\n","Loading cached processed dataset at /home/xu0494akk/.cache/huggingface/datasets/csv/data-9169fe6d1c846a71/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-e08a8b074cfc54cb.arrow\n"]}],"source":["# map the function to the whole dataset\n","processed_data = dataset.map(preprocess,    # your processing function\n","                             batched = True # Process in batches so it can be faster\n","                            )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_G5ftJbjfa4i","outputId":"c88d0fc1-85ca-4cdb-e191-a426e29fcfc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'level', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","        num_rows: 20720\n","    })\n","    test: Dataset({\n","        features: ['text', 'level', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","        num_rows: 2300\n","    })\n","})\n"]},{"data":{"text/plain":["{'text': 'My mother is having her car repaired.',\n"," 'level': 'B1',\n"," 'input_ids': [101, 2026, 2388, 2003, 2383, 2014, 2482, 13671, 1012, 102],\n"," 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n"," 'label': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["print(processed_data)\n","processed_data['train'][0]"]},{"cell_type":"markdown","metadata":{"id":"G9z7ZMtP22b9"},"source":["### DataCollator\n","\n","You might have noticed that we skipped padding the sentences. That's because we are going to do it during training.  \n","\n","To do training-time processing, we can use the DataCollator Class provided by `transformers`. And guess what - transformers has a class that will handle padding for us, too!\n","\n"," - [transformers.DataCollatorWithPadding](https://huggingface.co/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5orGYjN39dz"},"outputs":[],"source":["# [ TODO ] declare a collator to do padding during traning\n","from transformers import DataCollatorWithPadding\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"di75QVgv4V81"},"source":["## Training\n","\n","Finally, we can move on to training."]},{"cell_type":"markdown","metadata":{"id":"53dO3pg85u0n"},"source":["### Preparation\n","\n","We can load the pretrained model from `transformers`.  \n","Generally, you need to build your own model on top of BERT if you want to use BERT for some downstream tasks, but again, sequence classification is a popular topic. With the support from `transformers` library, it can be done in two lines of codes: \n","\n","1. Load `AutoModelForSequenceClassification` Class.\n","2. Load the pretrained model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UyDyv7wp5qdD","outputId":"068b8bdd-019a-4c9f-807d-a20ab385eb70"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,num_labels = LABEL_COUNT)"]},{"cell_type":"markdown","metadata":{"id":"DtmP4TEh6XiB"},"source":["#### Split train/val data\n","\n","The `Dataset` class we prepared before has a `train_test_split` method. You can use it to split your (processed) dataset.\n","\n","Document:\n"," - [datasets.Dataset - Sort, shuffle, select, split, and shard](https://huggingface.co/docs/datasets/process.html#sort-shuffle-select-split-and-shard)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NnbD1KW16YWn"},"outputs":[],"source":["# [ TODO ] choose a validation size and split your data\n","train_val_dataset = processed_data['train'].train_test_split(test_size = 0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6u93fCpofgbe","outputId":"65b7044c-6a01-4c07-f456-75d0e83d9d70"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'level', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","        num_rows: 16576\n","    })\n","    test: Dataset({\n","        features: ['text', 'level', 'input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","        num_rows: 4144\n","    })\n","})\n"]}],"source":["print(train_val_dataset)"]},{"cell_type":"markdown","metadata":{"id":"YzmSbOaD7O0F"},"source":["#### Setup training parameters\n","\n","We are using the TrainerAPI to do the training. Trainer is yet another utility provided by huggingface, which helps you train the model with ease.  \n","\n","Document:\n","- [transformers.TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments)\n","- [transformers.Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABqlinlO76Ax"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lzXTG1y7q7n","outputId":"e4599438-a1b9-40bf-c04b-b0f2dffd9b8d"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xu0494akk/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xu0494akk/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# [ TODO ] set and tune your training properties\n","OUTPUT_DIR = 'model'\n","LEARNING_RATE = 5e-5\n","BATCH_SIZE = 32\n","EPOCH = 5\n","training_args = TrainingArguments(\n","    output_dir = OUTPUT_DIR,\n","    learning_rate = LEARNING_RATE,\n","    per_device_train_batch_size = BATCH_SIZE,\n","    per_device_eval_batch_size = BATCH_SIZE,\n","    num_train_epochs = EPOCH,\n","    # you can set more parameters here if you want\n",")\n","\n","def model_init():\n","    return AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,num_labels = LABEL_COUNT)\n","\n","# now give all the information to a trainer\n","trainer = Trainer(\n","    model_init  = model_init,\n","    args = training_args,\n","    data_collator = data_collator,\n","    train_dataset = train_val_dataset['train'],\n","    eval_dataset = train_val_dataset['test'],\n","    tokenizer = tokenizer,\n","    # set your parameters here\n",")"]},{"cell_type":"markdown","metadata":{"id":"gE0DpS3s7rhg"},"source":["### Training\n","\n","This is the easy part. Simply ask the trainer to train the model for you!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wsrQOyJCiFas","outputId":"062f8db2-94b3-4da8-9511-43f10d417491"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xu0494akk/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xu0494akk/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: level, text. If level, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/home/xu0494akk/miniconda3/envs/py36/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 16576\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2590\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2590' max='2590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2590/2590 06:08, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.362700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.266600</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.178200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.116800</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.075300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to model/checkpoint-500\n","Configuration saved in model/checkpoint-500/config.json\n","Model weights saved in model/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in model/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in model/checkpoint-500/special_tokens_map.json\n","Saving model checkpoint to model/checkpoint-1000\n","Configuration saved in model/checkpoint-1000/config.json\n","Model weights saved in model/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in model/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in model/checkpoint-1000/special_tokens_map.json\n","Saving model checkpoint to model/checkpoint-1500\n","Configuration saved in model/checkpoint-1500/config.json\n","Model weights saved in model/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in model/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in model/checkpoint-1500/special_tokens_map.json\n","Saving model checkpoint to model/checkpoint-2000\n","Configuration saved in model/checkpoint-2000/config.json\n","Model weights saved in model/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in model/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in model/checkpoint-2000/special_tokens_map.json\n","Saving model checkpoint to model/checkpoint-2500\n","Configuration saved in model/checkpoint-2500/config.json\n","Model weights saved in model/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in model/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in model/checkpoint-2500/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=2590, training_loss=0.19522105128608616, metrics={'train_runtime': 373.9007, 'train_samples_per_second': 221.663, 'train_steps_per_second': 6.927, 'total_flos': 1960461546408576.0, 'train_loss': 0.19522105128608616, 'epoch': 5.0})"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"4JBUA0pe-S9b"},"source":["### Save for future use\n","\n","Hint: try using `save_pretrained`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l6WBg_q76utg"},"outputs":[],"source":["trained_model_name = f\"batch{str(BATCH_SIZE)}_epoch{str(EPOCH)}_lrate_{str(LEARNING_RATE)}\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7e30uXCf-cXc","outputId":"f003dfa4-060a-4419-9ba6-f559029e5402"},"outputs":[{"name":"stderr","output_type":"stream","text":["Configuration saved in model/batch32_epoch5_lrate_5e-05/config.json\n","Model weights saved in model/batch32_epoch5_lrate_5e-05/pytorch_model.bin\n"]}],"source":["# [ TODO ] practice saving your model for future use\n","model.save_pretrained(os.path.join('model', trained_model_name))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_pI0n5R6uth"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"7QSZjlcG9fOk"},"source":["## Prediction\n","\n","Now we know exactly how to train a model, but how do we use it for predicting results?"]},{"cell_type":"markdown","metadata":{"id":"O7akP5Hh9ugG"},"source":["### Load finetuned model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lfb_zJGm9vJP","outputId":"faaeaaea-e664-4792-84cc-f24d9f8656b5"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file model/batch32_epoch5_lrate_5e-05/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"model/batch32_epoch5_lrate_5e-05\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.18.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file model/batch32_epoch5_lrate_5e-05/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at model/batch32_epoch5_lrate_5e-05.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"]}],"source":["# [ TODO ] load the model that you saved\n","\n","from transformers import AutoModelForSequenceClassification\n","mymodel = AutoModelForSequenceClassification.from_pretrained(os.path.join('model', trained_model_name))"]},{"cell_type":"markdown","metadata":{"id":"a6Vs3iBE_nck"},"source":["### Get the prediction\n","\n","Here are a few example sentences:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FtLt6IBi_pLF"},"outputs":[],"source":["examples = [\n","    # A2\n","    \"Remember to write me a letter.\",\n","    # B2\n","    \"Strawberries and cream - a perfect combination.\",\n","    \"This so-called \\\"Perfect Evening\\\" was so disappointing, as well as discouraging us from coming to your Circle Theatre again.\",\n","    # C1\n","    \"Some may altogether give up their studies, which I think is a disastrous move.\",\n","]"]},{"cell_type":"markdown","metadata":{"id":"_xhk7ZRX_2U2"},"source":["All we need to do is to transform them to embeddings, and then we can get predictions by calling your finetuned model.  \n","\n","Since we don't have a DataCollator to pad the sentence and do the matrix transformation this time, we have to pad and transform the matrice on our own."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tWONG_lCAkyN","outputId":"0c408ae7-997d-4c0c-8f49-c6196397602f"},"outputs":[{"data":{"text/plain":["tensor([[ 0.0390,  0.2067,  0.3228, -0.1616, -0.5962, -0.3523],\n","        [ 0.0053,  0.3278,  0.2742, -0.0489, -0.5476, -0.3183],\n","        [ 0.1105,  0.1611,  0.3583, -0.1852, -0.5540, -0.3359],\n","        [ 0.0168,  0.0991,  0.3156, -0.1858, -0.5132, -0.2980]],\n","       grad_fn=<AddmmBackward0>)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# Transform the sentences into embeddings\n","input = tokenizer(examples, truncation=True, padding=True, return_tensors=\"pt\")\n","# Get the output\n","logits = mymodel(**input).logits\n","logits"]},{"cell_type":"markdown","metadata":{"id":"vBMUBD-1BFW1"},"source":["Logits aren't very readable for us. Let's use softmax \n","activation to transform them into more probability-like numbers."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjiKxLaBBGah","outputId":"006d24be-ed4f-48cb-ab2c-fa7b70b44dd0"},"outputs":[{"data":{"text/plain":["tensor([[0.1807, 0.2137, 0.2400, 0.1478, 0.0957, 0.1222],\n","        [0.1685, 0.2326, 0.2205, 0.1596, 0.0969, 0.1219],\n","        [0.1911, 0.2011, 0.2449, 0.1422, 0.0984, 0.1223],\n","        [0.1796, 0.1950, 0.2421, 0.1466, 0.1057, 0.1311]],\n","       grad_fn=<SoftmaxBackward0>)"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["from torch import nn\n","\n","predicts = nn.functional.softmax(logits, dim = -1)\n","predicts"]},{"cell_type":"markdown","metadata":{"id":"zAqgoJTFBchb"},"source":["#### Transform logits back to labels\n","\n","Now you've got the output. Write a function to map it back into labels!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvcyotBfBifR","outputId":"50cae16f-06ed-4f04-c684-496044f37233"},"outputs":[{"data":{"text/plain":["tensor([2, 1, 2, 2])"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# [ TODO ] try to process the result\n","predicts = torch.argmax(predicts, dim=1)\n","predicts"]},{"cell_type":"markdown","metadata":{"id":"gfCuP95IBvP-"},"source":["## Evaluation\n","\n","Let's see how you did!  \n","Load the testing data and calculate your accuracy.\n","\n","We want you to calculate the three kinds of accuracy mentioned in the lecture, which will also be explained in the following section."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNgzEWwE6utp","outputId":"c6571a3f-18b6-45a8-a1b1-e29005b232d0"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: level, text. If level, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 2300\n","  Batch size = 32\n"]},{"name":"stdout","output_type":"stream","text":["pred_level ['C2' 'B2' 'B2' 'B2' 'B2' 'A2' 'B1' 'B2' 'B1' 'C2']\n","label ['C2' 'B2' 'B2' 'C2' 'C1' 'A2' 'B1' 'B2' 'A2' 'C1']\n","level ['C2', 'B2', 'B2', 'C2', 'C1', 'A2', 'B1', 'B2', 'A2', 'C1']\n"]}],"source":["predictions, labels, metrics = trainer.predict(processed_data[\"test\"])\n","predictions = torch.from_numpy(predictions)\n","predictions_softmax = nn.functional.softmax(predictions, dim = -1)\n","\n","result = torch.argmax(predictions_softmax,dim=1).to(\"cuda:0\")\n","pred_labels = torch.nn.functional.one_hot(result)\n","pred_level = encoder.inverse_transform(pred_labels.cpu()).flatten()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ww27o0-Y6utq","outputId":"f825ed7c-8307-4a96-ea11-24e5f0a3de34"},"outputs":[{"name":"stdout","output_type":"stream","text":["C2: No longer a remote, backward, unimportant country, it became a force to be reckoned with in Europe.\n","B2: Unfortunately he was too fast and I couldn't keep up with him.\n","B2: Most mushrooms are totally harmless, but some are poisonous.\n","B2: This provided solid evidence that he committed the crime.\n","B2: You can't just accept everything you read in the newspapers at face value.\n","A2: Remember to write me a letter.\n","B1: She has long blond hair and blue eyes. She has a good figure.\n","B2: Nowadays the aim in clothing is not just for covering and protecting ourselves.\n","B1: Take two tablets, three times a day.\n","C2: Well, you will be if you saw our slide show and talk - members can hardly forget that relaxing afternoon when we unfolded the sails on the lake and enjoyed the tranquility of the area.\n"]}],"source":["for idx, (sent, level) in enumerate(zip(processed_data[\"test\"]['text'], pred_level)):\n","    if idx >= 10: break\n","    print(f'{level}: {sent}') "]},{"cell_type":"markdown","metadata":{"id":"PBnEzAFhC7ZN"},"source":["### Six Level Accuracy\n","\n","Exact accuracy is probably what you're most familiar with:\n","\n","$\n","accuracy = \\frac{\\#exactly\\:the\\:same\\:levels}{\\#total}\n","$\n","\n","Example:\n","```\n","Prediction:   A1 A2 B1 B2 C1 C2\n","Ground truth: A2 B1 B1 B2 B2 C2\n","                    ^  ^     ^\n","```\n","\n","The six level accuracy is $\\frac{3}{6} = 0.5$\n","\n","As the requirement, <u>your exact accuracy should be higher than $0.5$</u>."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kR2oVECyC8vD","outputId":"72cebbe0-5ada-446b-b5f7-6a9849292792"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5660869565217391\n"]}],"source":["# [ TODO ] calculate accuracy\n","\n","ground_truth = processed_data[\"test\"]['level']\n","\n","\n","correct = 0\n","for index, level in enumerate(pred_level):\n","    if level == ground_truth[index]:\n","        correct += 1\n","\n","print(correct/len(pred_level))\n"]},{"cell_type":"markdown","metadata":{"id":"We865ayVC_N7"},"source":["### Three Level Accuracy\n","\n","Three Level Accuracy is used when you only want a more general sense of right or wrong.\n","\n","$\n","accuracy = \\frac{\\#the\\:same\\:ABC\\:levels}{\\#total}\n","$\n","\n","Example:\n","```\n","Prediction:   A1 A2 B1 B2 C1 C2\n","Ground truth: A2 B1 B1 B2 B2 C2\n","              ^     ^  ^     ^\n","```\n","\n","The three level accuracy is $\\frac{4}{6} = 0.667$\n","\n","As the requirement, <u>your exact accuracy should be higher than $0.6$</u>."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCAKM9MRDCBk","outputId":"f599130e-de22-4b28-a521-f378afd52711"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7508695652173913\n"]}],"source":["# [ TODO ] calculate accuracy\n","ground_truth = processed_data[\"test\"]['level']\n","\n","\n","correct = 0\n","for index, level in enumerate(pred_level):\n","    if level[0] == ground_truth[index][0]:\n","        correct += 1\n","\n","print(correct/len(pred_level))\n"]},{"cell_type":"markdown","metadata":{"id":"3YSo46NX7nvb"},"source":["### Fuzzy accuracy\n","\n","However, the level of a sentence is relatively subjective. Generally speaking, $\\pm1$ errors are allowed in the real evaluation in linguistic area.  \n","\n","For example, if the actual label is 'B1', we'll also consider the prediction 'right' if the model predicts 'B2' or 'A2'.\n","\n","Hence, the fuzzy accuracy is\n","\n","$\n","accuracy = \\frac{\\#good\\:enough\\:answers}{\\#total}\n","$\n","\n","Example:\n","```\n","Prediction:   0 1 2 3 4 5\n","Ground truth: 0 1 1 3 3 3\n","              ^ ^ ^ ^ ^\n","```\n","\n","The fuzzy accuracy is $\\frac{5}{6} = 0.833$\n","\n","As the requirement, <u>your accuracy should be higher than $0.8$</u>."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"27fP0BTc73Al","outputId":"e792e699-4e04-4a82-ebff-d133aead0b60"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8843478260869565\n"]}],"source":["# [ TODO ] calculate accuracy\n","ground_truth = processed_data[\"test\"]['level']\n","categories = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n","\n","correct = 0\n","for index, level in enumerate(pred_level):\n","    if abs(categories.index(ground_truth[index]) - categories.index(level)) <= 1:\n","        correct += 1\n","\n","print(correct/len(pred_level))"]},{"cell_type":"markdown","metadata":{"id":"BPrSLQEDDfeE"},"source":["## TA's Note\n","\n","Congratulations, you made it to the end of the tutorial! Make sure you make an appointment to show your work and turn in your finished assignment before next week's lesson. We will ask you to run your code, so double check that everything is working and that your model is saved. Don't worry if you didn't pass the evaluation requirements, you'll still get partial points for trying."]},{"cell_type":"markdown","metadata":{"id":"aL7CAtRQbR7s"},"source":["## Appendix "]},{"cell_type":"markdown","metadata":{"id":"lqqUYYPEanAF"},"source":["\n","<a name=\"Appendix-1-Install-CUDA\"></a>\n","\n","### Appendix 1 - Install CUDA\n","\n","1. Check your GPU vs. CUDA compatibility:\n","   - [NVIDIA -> Your GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) -> GeForce and TITAN Products\n","2. Check library vs. CUDA compatibility: \n","   - Pytorch: [Previous PyTorch Versions](https://pytorch.org/get-started/previous-versions/)\n","   - Tensorflow: [Linux/MacOX](https://www.tensorflow.org/install/source#tested_build_configurations) or [Windows](https://www.tensorflow.org/install/source_windows#tested_build_configurations)\n","3. Note the highest CUDA version that fits your system.\n","\n","#### >> for conda/mamba users\n","\n","You can directly install CUDA library with the selected CUDA version.\n","1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n","2. `conda/mamba install -c conda-forge cudatoolkit=${VERSION}`\n","\n","#### >> for non-conda users\n","\n","1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n","2. Download and install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n","3. Download and install [cuDNN Library](https://developer.nvidia.com/rdp/cudnn-archive)"]},{"cell_type":"markdown","metadata":{"id":"8hFAM1Cya4_c"},"source":["### Appendix 2 - Further Readings\n","\n","1. [Huggingface Official Tutorials](https://github.com/huggingface/notebooks/tree/master/examples)\n","2. How to use Bert with other downstream tasks: [How to use BERT from the Hugging Face transformer library](https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209): \n","3. Training with pytorch backend: [transformers-tutorials](https://github.com/abhimishra91/transformers-tutorials)\n","4. A more complicated example that include manual data/training processing with Pytorch: [Transformers for Multi-Label Classification made simple](https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1)\n","5. [Text Classification with tensorflow](https://github.com/huggingface/notebooks/blob/master/examples/text_classification-tf.ipynb): tensorflow example"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.6.13 ('py36': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"vscode":{"interpreter":{"hash":"b96a47ac0ebb9d6b2a3c1c3989490ff9808843741bc47864810b1a2fb8797107"}}},"nbformat":4,"nbformat_minor":0}