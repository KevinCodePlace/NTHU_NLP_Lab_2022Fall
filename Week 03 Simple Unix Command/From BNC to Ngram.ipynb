{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f9b469",
   "metadata": {},
   "source": [
    "## From BNC to Ngram - clarification \n",
    "There seems to be a general sense of confusion regarding this homeword assignment. I would try to provide a more comprehensive instruction with some hints to handle some of the problems encountered during class.\n",
    "\n",
    "## Objective\n",
    "The end goal of this assignment is to generate the rank and rank ratio between BNC and clang8 for all the bigrams with the format \"adj. accident\"\n",
    "\n",
    "\n",
    "#### BNC Data:  \n",
    "https://drive.google.com/file/d/1mKX1DLHDIqKph4e4k1MnYOV3iWtvT7-E/view?usp=sharing\n",
    "\n",
    "#### Cleaned Lang 8 Data:\n",
    "https://drive.google.com/file/d/11wxKJr-VpmrHZ-MR41i4E097uY-T9okH/view?usp=sharing\n",
    "\n",
    "\n",
    "### A. Processing BNC Data\n",
    "### 1.1 Extract lines containing id, title, classcode, keywords, sentences from each BNC parts (ABCDEFGHJK)\n",
    "\n",
    "using grep / egrep to match regular expression and extract relavent data  \n",
    "\n",
    "Reference\n",
    "https://www.twblogs.net/a/5d26d705bd9eee1e5c84509d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc2d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "egrep -o -h  Texts/*/*/A*.xml > BNC.A.txt  237.80s user 1.29s system 99% cpu 4:00.15 total\r\n"
     ]
    }
   ],
   "source": [
    "# extract data from raw data\n",
    "! egrep -o -h \\\n",
    "'(<idno type=\"bnc\">.*?</idno>|<title>.*?</title>|<classCode.*?</classCode>|<keywords>.*?</keywords>|<s n=\".*?\">|<w c5=\".*?\" hw=\".*?\" pos=\".*?\">.*?</w>|<c c5=\".*?\">.*?</c>|</s>|<p>|</p>)' \\\n",
    "Texts/*/*/A*.xml | sed 's/></>\\n</g' > BNC.A.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc4b8f",
   "metadata": {},
   "source": [
    "**NOTE:** From the lab, it seems most systems output  \n",
    "\n",
    "\\<s n=\"1\"\\>\\<w c5=\"NN1\" hw=\"factsheet\" pos=\"SUBST\"\\>FACTSHEET \\</w\\>\\<w c5=\"DTQ\" hw=\"what\" pos=\"PRON\"\\>WHAT \\</w\\>\\<w c5=\"VBZ\" hw=\"be\" pos=\"VERB\"\\>IS \\</w\\>\\<w c5=\"NN1\" hw=\"aids\" pos=\"SUBST\"\\>AIDS\\</w\\>...\\</s\\>  \n",
    "  \n",
    "as one line. For line_to_token() to work, please sepereate each word (\\<w\\> ~ \\</w\\>) into one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e74d7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title> [ACET factsheets &amp; newsletters]. Sample containing about 6688 words of miscellanea (domain: social science) </title>\r\n",
      "<idno type=\"bnc\">A00</idno>\r\n",
      "<title> [ACET factsheets &amp; newsletters]. </title>\r\n",
      "<classCode scheme=\"DLEE\">W nonAc: medicine</classCode>\r\n",
      "<keywords><term> Health </term><term> Sex </term></keywords>\r\n",
      "<s n=\"1\">\r\n",
      "<w c5=\"NN1\" hw=\"factsheet\" pos=\"SUBST\">FACTSHEET </w>\r\n",
      "<w c5=\"DTQ\" hw=\"what\" pos=\"PRON\">WHAT </w>\r\n",
      "<w c5=\"VBZ\" hw=\"be\" pos=\"VERB\">IS </w>\r\n",
      "<w c5=\"NN1\" hw=\"aids\" pos=\"SUBST\">AIDS</w>\r\n"
     ]
    }
   ],
   "source": [
    "! head BNC.A.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20bcf30",
   "metadata": {},
   "source": [
    " ### 2. Convert sentences to bigram (for all sections A to K, no I)\n",
    " After you extract all the BNC data (BNC.A.txt, BNC.B.txt, BNC.C.txt ...), you need to process xml into tokens and bigrams.\n",
    " \n",
    " ### 2.1 Convert line to word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b00de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "BNC_bigram_total_number = 0 \n",
    "    \n",
    "def line_to_token(line):\n",
    "    BNC_bigram_total_number = 0\n",
    "    if line.startswith('<s'):\n",
    "        return ('<s> ', '<s>', '<s>') \n",
    "    elif line.startswith('</s'):\n",
    "        return ('</s>', '</s>', '</s>') \n",
    "    elif line.startswith('<w'):\n",
    "        # <w c5=\"VVN\" hw=\"discount\" pos=\"VERB\">discounted </w>\n",
    "        match = re.findall('<w c5=\"(.*?)\" hw=\"(.*?)\" pos=\".*?\">(.*?)</w>', line)\n",
    "        return (match[0][2].strip(), match[0][0].upper(), match[0][1]) #  , tag, word\n",
    "    elif line.startswith('<c'):\n",
    "        match = re.findall('<c c5=\"PUN\">(.*?)</c>', line)\n",
    "        if not match:\n",
    "            return '??? line'\n",
    "        return (match[0], match[0], match[0])\n",
    "\n",
    "def tokens_to_bigram(tokens):\n",
    "    result = []\n",
    "    for i in range(len(tokens)-1):\n",
    "        if i == 1:\n",
    "            word2tag2lemma2 = [tokens[i][j].lower()+' '+tokens[i+1][j] for j in range(3)]\n",
    "        else:\n",
    "            word2tag2lemma2 = [tokens[i][j]+' '+tokens[i+1][j] for j in range(3)]\n",
    "        if word2tag2lemma2[0][0].isalpha() or word2tag2lemma2[0][0] == '<': \n",
    "            result = result + [ '\\t'.join(word2tag2lemma2) ]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5eb1cc",
   "metadata": {},
   "source": [
    "### 2.2 Convert token stream to bigram stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7a275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13854202\n"
     ]
    }
   ],
   "source": [
    "def word_to_bigram(wordfile, bigramfile):\n",
    "    BNC_bigram_total_number = 0\n",
    "    def Batch_to_ngram(batch, fileout):        \n",
    "        with open(wordfile.format(batch),'r',encoding=\"utf-8\") as filein:\n",
    "            lines = filein.readlines()\n",
    "            count = 1\n",
    "            last = 0\n",
    "            for i, line in enumerate(lines):\n",
    "                if line.startswith('<s'):\n",
    "                    sent_start = i\n",
    "                    count = count + 1\n",
    "                elif line.startswith('</s'):\n",
    "                    sentence = [line.strip() for line in lines[sent_start:i+1]]\n",
    "                    tokens = [line_to_token(line) for line in sentence ]\n",
    "                    bigram = tokens_to_bigram(tokens)\n",
    "                    count = count + 1\n",
    "                    last = i\n",
    "                    print('\\n'.join(bigram), file=fileout)\n",
    "            BNC_bigram_total_number = last-count\n",
    "            return BNC_bigram_total_number\n",
    "    \n",
    "    with open(bigramfile, 'w',encoding=\"utf-8\") as fileout:\n",
    "        for batch in 'A':\n",
    "            BNC_bigram_total_number = Batch_to_ngram(batch, fileout)\n",
    "    return BNC_bigram_total_number\n",
    "                \n",
    "BNC_bigram_total_number = word_to_bigram('BNC.{0}.txt', 'BNC.2w.txt')\n",
    "print(BNC_bigram_total_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadce87",
   "metadata": {},
   "source": [
    "### 3 Sort and count bigram (word1 word2 \\<tab\\> count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b268aac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort: No such file or directory\r\n",
      "sort BNC.B.2w.txt  0.00s user 0.00s system 54% cpu 0.005 total\r\n",
      "uniq -c  0.00s user 0.00s system 49% cpu 0.004 total\r\n",
      "awk '{ gsub(/^[ ]*/, \"\"); print }'  0.00s user 0.00s system 49% cpu 0.004 total\r\n",
      "awk '{print substr($0, index($0, \" \")+1) \"\\t\" $1}'  0.00s user 0.00s system 52% cpu 0.004 total\r\n",
      "egrep -v '\\t1$' > BNC.B.2w.c2+.txt  0.00s user 0.00s system 56% cpu 0.003 total\r\n"
     ]
    }
   ],
   "source": [
    "#1 BNC.2w.txt ==> BNC.2w.c.txt\n",
    "# We sort the bigrams and count identical bigrams\n",
    "# NOTE: BNC.2w.c.txt should be considerably smaller than BNC.2w.txt\n",
    "# NOTE: Since we only care about \"adj. accident\", you can filter BNC.2w.c.txt into a much\n",
    "#       smaller file by extracting bigrams that fit \"adj. accident\"\n",
    "\n",
    "! time sort BNC.2w.txt | uniq -c | \\\n",
    "awk '{ gsub(/^[ ]*/, \"\"); print }' | awk '{print substr($0, index($0, \" \")+1) \"\\t\" $1}' > BNC.2w.c.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "12341e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big accident\tAJ0 NN1\tbig accident\t3\n",
      "fatal accident\tAJ0 NN1\tfatal accident\t82\n",
      "fatal accident\taj0 NN1\tfatal accident\t3\n",
      "serious accident\tAJ0 NN1\tserious accident\t61\n"
     ]
    }
   ],
   "source": [
    "# Example data format for BNC.2w.c.txt\n",
    "# bigram       pos      lemmas          count\n",
    "\n",
    "! egrep '^(big|serious|fatal) accident\\t' BNC.2w.c.txt\n",
    "! egrep 'accident\\s.*AJ' BNC.2w.c.txt > adj_accident.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a0c2b",
   "metadata": {},
   "source": [
    "### B. Processing clang8 Data\n",
    "During the lab, we found out that the lang8 dataset we provided last week did not contain enough \"adj. accident\" bigrams for this assignment.  \n",
    "Please process the clang8 data provided above and extract bigrams.  \n",
    "**NOTE:** We are only intereted in the rank / rank ratio for  \"adj. accident\" bigrams. Please extract bigrams of \"adj. accident\" from both BNC and clang8 first, then calculate the rank and rank ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d5117",
   "metadata": {},
   "source": [
    "Target output format: \n",
    "https://drive.google.com/file/d/1xM46aaDIeu4Z0FkikGOcmDoq7u2O47tY/view?usp=sharing\n",
    "\n",
    "Demo time sign up for Lab 3:\n",
    "https://docs.google.com/spreadsheets/d/1OKbXhcv6E3FEQDPnbHEHEeHvpxv01jxugMP7WwnKqKw/edit?usp=sharing\n",
    "\n",
    "For Demo, please print out all the \"adj. accident\" bigrams in descending rank ratio order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99739b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import copy\n",
    "from pprint import pprint\n",
    "\n",
    "import itertools\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24fa5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    \"This is an example.'\n",
    "\n",
    "    Sample output: \n",
    "    ['this', 'is', 'an', 'example', '.']\n",
    "    \"\"\"  \n",
    "    #### [ TODO ] transform text to lower case\n",
    "    lowerText = text.lower()\n",
    "    pureText = re.sub(r'[.,\"\\'-?:!;]', '', re.sub(r'[0-9]+', '', lowerText))\n",
    "    #### [ TODO ] seperate the words by white space\n",
    "    splitText = pureText.split()\n",
    "    return splitText\n",
    "    \n",
    "from collections import Counter\n",
    "\n",
    "def calculate_frequency(tokens):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    ['this', 'is', 'an', 'example', ...]\n",
    "\n",
    "    Sample output: \n",
    "    {\n",
    "        'the': 79809, \n",
    "        'project': 288,\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    #### [ TODO ] \n",
    "    topToken = Counter(tokens).most_common()\n",
    "    di = dict(topToken)\n",
    "    return di\n",
    "   \n",
    "def ranking(tokens):\n",
    "    rank = 0\n",
    "    previous_number = 0\n",
    "    for element in tokens:\n",
    "        if(tokens[element] != previous_number):\n",
    "            rank += 1\n",
    "        previous_number = tokens[element]\n",
    "        tokens[element] = rank\n",
    "    return tokens\n",
    "\n",
    "def get_ngram(tokens,n):\n",
    "    ngram_result = []\n",
    "    ngram_list = [tokens[i:i+n] for i in range(0,len(tokens)-1)]\n",
    "    for element in ngram_list:\n",
    "        ngram_result.append(' '.join(element[0 : n]))\n",
    "    return ngram_result\n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    ['this', 'is', 'an', 'example', ...]\n",
    "\n",
    "    Sample output: \n",
    "    ['this is', 'is an', 'an example', ...]\n",
    "    \"\"\"\n",
    "    #### [TODO] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2be0d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BNC Processing\n",
    "f = open('adj_accident.txt', 'r',encoding=\"utf-8\")\n",
    "line = f.readline()\n",
    "BNC_bigram_counter = {}\n",
    "while line:\n",
    "    BNC_readline = line.split()\n",
    "    BNC_dic_key = BNC_readline[4] +' '+BNC_readline[5]\n",
    "    BNC_dic_value = int(BNC_readline[6])\n",
    "    BNC_bigram_counter[BNC_dic_key]=BNC_dic_value\n",
    "    line = f.readline()\n",
    "\n",
    "list_BNC_bigram_counter = sorted(BNC_bigram_counter.items(), key=lambda kv: kv[1],reverse=True)\n",
    "BNC_bigram_counter = dict(list_BNC_bigram_counter)\n",
    "maintain_BNC_bigram_counter = copy.deepcopy(BNC_bigram_counter)\n",
    "BNC_bigram_Rank = ranking(BNC_bigram_counter)\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b157e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('clang8.txt', 'r',encoding=\"utf-8\")\n",
    "fileRead = f.read()\n",
    "#### [ TODO ] generate lang8 unigrams and calculate document frequency of unigram in lang8\n",
    "clang_unigram = tokenize(fileRead)\n",
    "f.close()\n",
    "\n",
    "clang_bigram = get_ngram(clang_unigram,2)\n",
    "clang_bigram_total_number = len(clang_bigram)\n",
    "clang_bigram_counter = calculate_frequency(clang_bigram)\n",
    "clang_bigram_accident_counter = {}\n",
    "\n",
    "for element in BNC_bigram_counter:\n",
    "    if(element in clang_bigram_counter):\n",
    "        clang_bigram_accident_counter[element]=clang_bigram_counter[element]\n",
    "list_clang_bigram_counter = sorted(clang_bigram_accident_counter.items(), key=lambda kv: kv[1],reverse=True)\n",
    "clang_bigram_accident_counter = dict(list_clang_bigram_counter)\n",
    "maintain_clang_bigram_accident_counter = copy.deepcopy(clang_bigram_accident_counter)\n",
    "clang_bigram_Rank = ranking(clang_bigram_accident_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f58c0126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'serious accident': 20, 'fatal accident': 8, 'nuclear accident': 7, 'personal accident': 7, 'historical accident': 5, 'disabling accident': 3, 'industrial accident': 3, 'lucky accident': 3, 'mere accident': 3, 'catastrophic accident': 2, 'dreadful accident': 2, 'major accident': 2, 'minor accident': 2, 'sad accident': 2, 'similar accident': 2, 'terrible accident': 2, 'tragic accident': 2, 'british accident': 1, 'accident black': 1, 'accident bad': 1, 'appalling accident': 1, 'awful accident': 1, 'bad accident': 1, 'big accident': 1, 'cerebrovascular accident': 1, 'collusive accident': 1, 'common accident': 1, 'complete accident': 1, 'conceivable accident': 1, 'diving accident': 1, 'entertaining accident': 1, 'exceptional accident': 1, 'flying accident': 1, 'follow accident': 1, 'freak accident': 1, 'free accident': 1, 'full accident': 1, 'far accident': 1, 'heavy accident': 1, 'hit-and-run accident': 1, 'huge accident': 1, 'immediate accident': 1, 'likely accident': 1, 'little accident': 1, 'ludicrous accident': 1, 'medical accident': 1, 'nasty accident': 1, 'near accident': 1, 'normal accident': 1, 'only accident': 1, 'original accident': 1, 'painful accident': 1, 'particular accident': 1, 'pure accident': 1, 'recent accident': 1, 'related accident': 1, 'seeming accident': 1, 'severe accident': 1, 'sheer accident': 1, 'shooting accident': 1, 'skating accident': 1, 'specific accident': 1, 'spectacular accident': 1, 'strange accident': 1, 'stupid accident': 1, 'unfortunate accident': 1, 'unlucky accident': 1, 'unsurvivable accident': 1}\n",
      "-----------------------\n",
      "{'serious accident': 1, 'fatal accident': 2, 'nuclear accident': 3, 'personal accident': 3, 'historical accident': 4, 'disabling accident': 5, 'industrial accident': 5, 'lucky accident': 5, 'mere accident': 5, 'catastrophic accident': 6, 'dreadful accident': 6, 'major accident': 6, 'minor accident': 6, 'sad accident': 6, 'similar accident': 6, 'terrible accident': 6, 'tragic accident': 6, 'british accident': 7, 'accident black': 7, 'accident bad': 7, 'appalling accident': 7, 'awful accident': 7, 'bad accident': 7, 'big accident': 7, 'cerebrovascular accident': 7, 'collusive accident': 7, 'common accident': 7, 'complete accident': 7, 'conceivable accident': 7, 'diving accident': 7, 'entertaining accident': 7, 'exceptional accident': 7, 'flying accident': 7, 'follow accident': 7, 'freak accident': 7, 'free accident': 7, 'full accident': 7, 'far accident': 7, 'heavy accident': 7, 'hit-and-run accident': 7, 'huge accident': 7, 'immediate accident': 7, 'likely accident': 7, 'little accident': 7, 'ludicrous accident': 7, 'medical accident': 7, 'nasty accident': 7, 'near accident': 7, 'normal accident': 7, 'only accident': 7, 'original accident': 7, 'painful accident': 7, 'particular accident': 7, 'pure accident': 7, 'recent accident': 7, 'related accident': 7, 'seeming accident': 7, 'severe accident': 7, 'sheer accident': 7, 'shooting accident': 7, 'skating accident': 7, 'specific accident': 7, 'spectacular accident': 7, 'strange accident': 7, 'stupid accident': 7, 'unfortunate accident': 7, 'unlucky accident': 7, 'unsurvivable accident': 7}\n",
      "-----------------------\n",
      "{'nuclear accident': 56, 'big accident': 21, 'terrible accident': 20, 'bad accident': 18, 'serious accident': 13, 'little accident': 7, 'fatal accident': 6, 'tragic accident': 5, 'unlucky accident': 5, 'lucky accident': 3, 'pure accident': 3, 'industrial accident': 2, 'minor accident': 2, 'sad accident': 2, 'similar accident': 2, 'awful accident': 2, 'cerebrovascular accident': 2, 'huge accident': 2, 'recent accident': 2, 'historical accident': 1, 'mere accident': 1, 'catastrophic accident': 1, 'major accident': 1, 'accident bad': 1, 'normal accident': 1, 'only accident': 1, 'strange accident': 1, 'unfortunate accident': 1}\n",
      "-----------------------\n",
      "{'nuclear accident': 1, 'big accident': 2, 'terrible accident': 3, 'bad accident': 4, 'serious accident': 5, 'little accident': 6, 'fatal accident': 7, 'tragic accident': 8, 'unlucky accident': 8, 'lucky accident': 9, 'pure accident': 9, 'industrial accident': 10, 'minor accident': 10, 'sad accident': 10, 'similar accident': 10, 'awful accident': 10, 'cerebrovascular accident': 10, 'huge accident': 10, 'recent accident': 10, 'historical accident': 11, 'mere accident': 11, 'catastrophic accident': 11, 'major accident': 11, 'accident bad': 11, 'normal accident': 11, 'only accident': 11, 'strange accident': 11, 'unfortunate accident': 11}\n"
     ]
    }
   ],
   "source": [
    "print(maintain_BNC_bigram_counter)\n",
    "print('-----------------------')\n",
    "print(BNC_bigram_Rank)\n",
    "print('-----------------------')\n",
    "print(maintain_clang_bigram_accident_counter)\n",
    "print('-----------------------')\n",
    "print(clang_bigram_Rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87eafe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases                     Overuse rank/rank  BNC rank(#/m)    LANG-8 rank(#/m)\n",
      "------------------------  -------------------  ---------------  ------------------\n",
      "serious accident                     0.2       1(1.44)          5(0.54)\n",
      "fatal accident                       0.285714  2(0.58)          7(0.25)\n",
      "historical accident                  0.363636  4(0.36)          11(0.04)\n",
      "mere accident                        0.454545  5(0.22)          11(0.04)\n",
      "industrial accident                  0.5       5(0.22)          10(0.08)\n",
      "catastrophic accident                0.545455  6(0.14)          11(0.04)\n",
      "major accident                       0.545455  6(0.14)          11(0.04)\n",
      "lucky accident                       0.555556  5(0.22)          9(0.12)\n",
      "minor accident                       0.6       6(0.14)          10(0.08)\n",
      "sad accident                         0.6       6(0.14)          10(0.08)\n",
      "similar accident                     0.6       6(0.14)          10(0.08)\n",
      "accident bad                         0.636364  7(0.07)          11(0.04)\n",
      "normal accident                      0.636364  7(0.07)          11(0.04)\n",
      "only accident                        0.636364  7(0.07)          11(0.04)\n",
      "strange accident                     0.636364  7(0.07)          11(0.04)\n",
      "unfortunate accident                 0.636364  7(0.07)          11(0.04)\n",
      "awful accident                       0.7       7(0.07)          10(0.08)\n",
      "cerebrovascular accident             0.7       7(0.07)          10(0.08)\n",
      "huge accident                        0.7       7(0.07)          10(0.08)\n",
      "recent accident                      0.7       7(0.07)          10(0.08)\n",
      "tragic accident                      0.75      6(0.14)          8(0.21)\n",
      "pure accident                        0.777778  7(0.07)          9(0.12)\n",
      "unlucky accident                     0.875     7(0.07)          8(0.21)\n",
      "little accident                      1.16667   7(0.07)          6(0.29)\n",
      "bad accident                         1.75      7(0.07)          4(0.74)\n",
      "terrible accident                    2         6(0.14)          3(0.83)\n",
      "nuclear accident                     3         3(0.51)          1(2.31)\n",
      "big accident                         3.5       7(0.07)          2(0.87)\n"
     ]
    }
   ],
   "source": [
    "#### [ TODO ] Calculate Rank Ratio\n",
    "BNC_rank_ratio = {}\n",
    "for element in BNC_bigram_Rank:\n",
    "    if(element in clang_bigram_Rank):\n",
    "        BNC_rank_ratio[element] = (BNC_bigram_Rank[element]/clang_bigram_Rank[element])\n",
    "\n",
    "list_BNC_rank_ratio = sorted(BNC_rank_ratio.items(), key=lambda x: x[1])\n",
    "for i,word_rank_ratio in enumerate(list_BNC_rank_ratio):\n",
    "    BNC_usage_ratio = (maintain_BNC_bigram_counter[word_rank_ratio[0]] / BNC_bigram_total_number) * 1000000 \n",
    "    BNC_usage_ratio = \"(%.2f)\" % BNC_usage_ratio\n",
    "    clang_usage_ratio = (maintain_clang_bigram_accident_counter[word_rank_ratio[0]] / clang_bigram_total_number) * 1000000\n",
    "    clang_usage_ratio = \"(%.2f)\" % clang_usage_ratio\n",
    "    BNC_lang_rank = (str(BNC_bigram_Rank[word_rank_ratio[0]])+BNC_usage_ratio,str(clang_bigram_Rank[word_rank_ratio[0]])+clang_usage_ratio)\n",
    "    list_BNC_rank_ratio[i] = list_BNC_rank_ratio[i] + BNC_lang_rank\n",
    "\n",
    "headers = [\"Phrases\", \"Overuse rank/rank\",\"BNC rank(#/m)\",\"LANG-8 rank(#/m)\"]\n",
    "print(tabulate( list_BNC_rank_ratio, headers = headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e54c81b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54c216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
